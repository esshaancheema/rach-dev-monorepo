apiVersion: v1
kind: ConfigMap
metadata:
  name: backup-config
  namespace: zoptal-production
data:
  backup.conf: |
    # Backup Configuration for Zoptal Platform
    
    # Database backup settings
    DB_BACKUP_SCHEDULE="0 2 * * *"  # Daily at 2 AM
    DB_BACKUP_RETENTION_DAYS=30
    DB_BACKUP_LOCATION="s3://zoptal-backups/database"
    
    # Redis backup settings
    REDIS_BACKUP_SCHEDULE="0 */6 * * *"  # Every 6 hours
    REDIS_BACKUP_RETENTION_DAYS=7
    REDIS_BACKUP_LOCATION="s3://zoptal-backups/redis"
    
    # Application data backup
    APP_BACKUP_SCHEDULE="0 1 * * 0"  # Weekly on Sunday at 1 AM
    APP_BACKUP_RETENTION_WEEKS=4
    APP_BACKUP_LOCATION="s3://zoptal-backups/application"
    
    # File storage backup
    FILES_SYNC_SCHEDULE="0 */4 * * *"  # Every 4 hours
    FILES_BACKUP_LOCATION="s3://zoptal-backups/files"
    
    # Configuration backup
    CONFIG_BACKUP_SCHEDULE="0 3 * * *"  # Daily at 3 AM
    CONFIG_BACKUP_LOCATION="s3://zoptal-backups/config"

---
apiVersion: v1
kind: Secret
metadata:
  name: backup-credentials
  namespace: zoptal-production
type: Opaque
data:
  aws-access-key-id: QUtJQVNBTVBMRUtFWQ== # Base64 encoded
  aws-secret-access-key: U2FtcGxlU2VjcmV0QWNjZXNzS2V5 # Base64 encoded
  postgres-password: UG9zdGdyZXNTdXBlclNlY3VyZTEyMw== # Base64 encoded
  redis-password: UmVkaXMhMjAyNDEyMw== # Base64 encoded

---
# PostgreSQL Backup CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
  namespace: zoptal-production
  labels:
    app: postgres-backup
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  concurrencyPolicy: Forbid
  startingDeadlineSeconds: 300
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 3600  # 1 hour timeout
      template:
        metadata:
          labels:
            app: postgres-backup
        spec:
          restartPolicy: Never
          serviceAccountName: backup-service-account
          containers:
          - name: postgres-backup
            image: postgres:15-alpine
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: backup-credentials
                  key: postgres-password
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: backup-credentials
                  key: aws-access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: backup-credentials
                  key: aws-secret-access-key
            command:
            - /bin/bash
            - -c
            - |
              set -euo pipefail
              
              echo "Starting PostgreSQL backup at $(date)"
              
              # Create timestamp
              TIMESTAMP=$(date +%Y%m%d_%H%M%S)
              BACKUP_FILE="postgres_backup_${TIMESTAMP}.sql"
              
              # Backup all databases
              pg_dumpall -h postgres-service -p 5432 -U postgres > "/tmp/${BACKUP_FILE}"
              
              # Compress backup
              gzip "/tmp/${BACKUP_FILE}"
              BACKUP_FILE="${BACKUP_FILE}.gz"
              
              # Install AWS CLI
              apk add --no-cache aws-cli
              
              # Upload to S3
              aws s3 cp "/tmp/${BACKUP_FILE}" "s3://zoptal-backups/database/${BACKUP_FILE}"
              
              # Cleanup old backups (keep last 30 days)
              aws s3 ls s3://zoptal-backups/database/ | while read -r line; do
                createDate=$(echo "$line" | awk '{print $1" "$2}')
                createDate=$(date -d "$createDate" +%s)
                olderThan=$(date -d '30 days ago' +%s)
                if [[ $createDate -lt $olderThan ]]; then
                  fileName=$(echo "$line" | awk '{print $4}')
                  if [[ $fileName != "" ]]; then
                    aws s3 rm "s3://zoptal-backups/database/$fileName"
                    echo "Deleted old backup: $fileName"
                  fi
                fi
              done
              
              echo "PostgreSQL backup completed successfully"
            resources:
              requests:
                memory: "256Mi"
                cpu: "100m"
              limits:
                memory: "1Gi"
                cpu: "500m"
            volumeMounts:
            - name: backup-volume
              mountPath: /tmp
          volumes:
          - name: backup-volume
            emptyDir:
              sizeLimit: 10Gi

---
# Redis Backup CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: redis-backup
  namespace: zoptal-production
  labels:
    app: redis-backup
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  concurrencyPolicy: Forbid
  startingDeadlineSeconds: 300
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 2
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 1800  # 30 minutes timeout
      template:
        metadata:
          labels:
            app: redis-backup
        spec:
          restartPolicy: Never
          serviceAccountName: backup-service-account
          containers:
          - name: redis-backup
            image: redis:7.2-alpine
            env:
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: backup-credentials
                  key: redis-password
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: backup-credentials
                  key: aws-access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: backup-credentials
                  key: aws-secret-access-key
            command:
            - /bin/bash
            - -c
            - |
              set -euo pipefail
              
              echo "Starting Redis backup at $(date)"
              
              # Install AWS CLI
              apk add --no-cache aws-cli
              
              # Create timestamp
              TIMESTAMP=$(date +%Y%m%d_%H%M%S)
              
              # Get Redis nodes
              REDIS_NODES=$(redis-cli -h redis-cluster-client -p 6379 --no-auth-warning cluster nodes | grep master | awk '{print $2}' | cut -d: -f1)
              
              for node in $REDIS_NODES; do
                echo "Backing up Redis node: $node"
                
                # Save Redis data
                redis-cli -h "$node" -p 6379 --no-auth-warning BGSAVE
                
                # Wait for background save to complete
                while [ "$(redis-cli -h "$node" -p 6379 --no-auth-warning INFO persistence | grep rdb_bgsave_in_progress:1)" ]; do
                  echo "Waiting for BGSAVE to complete..."
                  sleep 5
                done
                
                # Copy RDB file
                BACKUP_FILE="redis_${node}_${TIMESTAMP}.rdb"
                redis-cli -h "$node" -p 6379 --no-auth-warning --rdb "/tmp/${BACKUP_FILE}"
                
                # Compress and upload
                gzip "/tmp/${BACKUP_FILE}"
                aws s3 cp "/tmp/${BACKUP_FILE}.gz" "s3://zoptal-backups/redis/${BACKUP_FILE}.gz"
                
                rm -f "/tmp/${BACKUP_FILE}.gz"
              done
              
              # Cleanup old backups (keep last 7 days)
              aws s3 ls s3://zoptal-backups/redis/ | while read -r line; do
                createDate=$(echo "$line" | awk '{print $1" "$2}')
                createDate=$(date -d "$createDate" +%s)
                olderThan=$(date -d '7 days ago' +%s)
                if [[ $createDate -lt $olderThan ]]; then
                  fileName=$(echo "$line" | awk '{print $4}')
                  if [[ $fileName != "" ]]; then
                    aws s3 rm "s3://zoptal-backups/redis/$fileName"
                    echo "Deleted old Redis backup: $fileName"
                  fi
                fi
              done
              
              echo "Redis backup completed successfully"
            resources:
              requests:
                memory: "128Mi"
                cpu: "100m"
              limits:
                memory: "512Mi"
                cpu: "300m"
            volumeMounts:
            - name: backup-volume
              mountPath: /tmp
          volumes:
          - name: backup-volume
            emptyDir:
              sizeLimit: 5Gi

---
# Configuration Backup CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: config-backup
  namespace: zoptal-production
  labels:
    app: config-backup
spec:
  schedule: "0 3 * * *"  # Daily at 3 AM
  concurrencyPolicy: Forbid
  startingDeadlineSeconds: 300
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 2
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 1200  # 20 minutes timeout
      template:
        metadata:
          labels:
            app: config-backup
        spec:
          restartPolicy: Never
          serviceAccountName: backup-service-account
          containers:
          - name: config-backup
            image: bitnami/kubectl:latest
            env:
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: backup-credentials
                  key: aws-access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: backup-credentials
                  key: aws-secret-access-key
            command:
            - /bin/bash
            - -c
            - |
              set -euo pipefail
              
              echo "Starting configuration backup at $(date)"
              
              # Install AWS CLI
              curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
              unzip awscliv2.zip
              ./aws/install
              
              # Create timestamp
              TIMESTAMP=$(date +%Y%m%d_%H%M%S)
              BACKUP_DIR="/tmp/config_backup_${TIMESTAMP}"
              mkdir -p "$BACKUP_DIR"
              
              # Export ConfigMaps
              echo "Exporting ConfigMaps..."
              kubectl get configmaps -n zoptal-production -o yaml > "$BACKUP_DIR/configmaps.yaml"
              
              # Export Secrets (without data for security)
              echo "Exporting Secret metadata..."
              kubectl get secrets -n zoptal-production -o yaml | \
                kubectl neat | \
                yq eval 'del(.items[].data)' - > "$BACKUP_DIR/secrets.yaml"
              
              # Export Services
              echo "Exporting Services..."
              kubectl get services -n zoptal-production -o yaml > "$BACKUP_DIR/services.yaml"
              
              # Export Deployments
              echo "Exporting Deployments..."
              kubectl get deployments -n zoptal-production -o yaml > "$BACKUP_DIR/deployments.yaml"
              
              # Export StatefulSets
              echo "Exporting StatefulSets..."
              kubectl get statefulsets -n zoptal-production -o yaml > "$BACKUP_DIR/statefulsets.yaml"
              
              # Export PersistentVolumes
              echo "Exporting PersistentVolumes..."
              kubectl get pv -o yaml > "$BACKUP_DIR/persistentvolumes.yaml"
              
              # Export PersistentVolumeClaims
              echo "Exporting PersistentVolumeClaims..."
              kubectl get pvc -n zoptal-production -o yaml > "$BACKUP_DIR/persistentvolumeclaims.yaml"
              
              # Export Ingresses
              echo "Exporting Ingresses..."
              kubectl get ingress -n zoptal-production -o yaml > "$BACKUP_DIR/ingresses.yaml"
              
              # Export NetworkPolicies
              echo "Exporting NetworkPolicies..."
              kubectl get networkpolicies -n zoptal-production -o yaml > "$BACKUP_DIR/networkpolicies.yaml"
              
              # Export HPAs
              echo "Exporting HPAs..."
              kubectl get hpa -n zoptal-production -o yaml > "$BACKUP_DIR/hpas.yaml"
              
              # Export CronJobs
              echo "Exporting CronJobs..."
              kubectl get cronjobs -n zoptal-production -o yaml > "$BACKUP_DIR/cronjobs.yaml"
              
              # Create backup archive
              cd /tmp
              tar -czf "config_backup_${TIMESTAMP}.tar.gz" "config_backup_${TIMESTAMP}"
              
              # Upload to S3
              aws s3 cp "config_backup_${TIMESTAMP}.tar.gz" "s3://zoptal-backups/config/config_backup_${TIMESTAMP}.tar.gz"
              
              # Cleanup old backups (keep last 30 days)
              aws s3 ls s3://zoptal-backups/config/ | while read -r line; do
                createDate=$(echo "$line" | awk '{print $1" "$2}')
                createDate=$(date -d "$createDate" +%s)
                olderThan=$(date -d '30 days ago' +%s)
                if [[ $createDate -lt $olderThan ]]; then
                  fileName=$(echo "$line" | awk '{print $4}')
                  if [[ $fileName != "" ]]; then
                    aws s3 rm "s3://zoptal-backups/config/$fileName"
                    echo "Deleted old config backup: $fileName"
                  fi
                fi
              done
              
              echo "Configuration backup completed successfully"
            resources:
              requests:
                memory: "256Mi"
                cpu: "100m"
              limits:
                memory: "512Mi"
                cpu: "300m"
            volumeMounts:
            - name: backup-volume
              mountPath: /tmp
          volumes:
          - name: backup-volume
            emptyDir:
              sizeLimit: 2Gi

---
# Service Account for backup operations
apiVersion: v1
kind: ServiceAccount
metadata:
  name: backup-service-account
  namespace: zoptal-production

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: backup-cluster-role
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets", "services", "persistentvolumes", "persistentvolumeclaims"]
  verbs: ["get", "list"]
- apiGroups: ["apps"]
  resources: ["deployments", "statefulsets"]
  verbs: ["get", "list"]
- apiGroups: ["networking.k8s.io"]
  resources: ["ingresses", "networkpolicies"]
  verbs: ["get", "list"]
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list"]
- apiGroups: ["batch"]
  resources: ["cronjobs"]
  verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: backup-cluster-role-binding
subjects:
- kind: ServiceAccount
  name: backup-service-account
  namespace: zoptal-production
roleRef:
  kind: ClusterRole
  name: backup-cluster-role
  apiGroup: rbac.authorization.k8s.io

---
# Backup monitoring and alerting
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: backup-monitoring
  namespace: zoptal-production
  labels:
    app: backup-monitoring
spec:
  selector:
    matchLabels:
      app: backup-exporter
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

---
# Backup status exporter
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backup-exporter
  namespace: zoptal-production
  labels:
    app: backup-exporter
spec:
  replicas: 1
  selector:
    matchLabels:
      app: backup-exporter
  template:
    metadata:
      labels:
        app: backup-exporter
    spec:
      serviceAccountName: backup-service-account
      containers:
      - name: backup-exporter
        image: prom/node-exporter:latest
        ports:
        - containerPort: 9100
          name: metrics
        env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: backup-credentials
              key: aws-access-key-id
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: backup-credentials
              key: aws-secret-access-key
        command:
        - /bin/sh
        - -c
        - |
          # Install AWS CLI
          apk add --no-cache aws-cli python3 py3-pip
          pip3 install prometheus_client
          
          # Create custom exporter script
          cat > /tmp/backup_exporter.py << 'EOF'
          import time
          import boto3
          import json
          from datetime import datetime, timedelta
          from prometheus_client import start_http_server, Gauge, Info
          
          # Metrics
          backup_success = Gauge('backup_last_success_timestamp', 'Last successful backup timestamp', ['backup_type'])
          backup_size = Gauge('backup_size_bytes', 'Size of last backup in bytes', ['backup_type'])
          backup_count = Gauge('backup_total_count', 'Total number of backups', ['backup_type'])
          
          def collect_backup_metrics():
              s3 = boto3.client('s3')
              bucket = 'zoptal-backups'
              
              backup_types = ['database', 'redis', 'config']
              
              for backup_type in backup_types:
                  try:
                      response = s3.list_objects_v2(Bucket=bucket, Prefix=f'{backup_type}/')
                      
                      if 'Contents' in response:
                          objects = response['Contents']
                          objects.sort(key=lambda x: x['LastModified'], reverse=True)
                          
                          if objects:
                              latest = objects[0]
                              backup_success.labels(backup_type=backup_type).set(latest['LastModified'].timestamp())
                              backup_size.labels(backup_type=backup_type).set(latest['Size'])
                              backup_count.labels(backup_type=backup_type).set(len(objects))
                  except Exception as e:
                      print(f"Error collecting metrics for {backup_type}: {e}")
          
          if __name__ == '__main__':
              start_http_server(9100)
              while True:
                  collect_backup_metrics()
                  time.sleep(300)  # Update every 5 minutes
          EOF
          
          # Run the exporter
          python3 /tmp/backup_exporter.py
        resources:
          requests:
            memory: "128Mi"
            cpu: "50m"
          limits:
            memory: "256Mi"
            cpu: "100m"

---
apiVersion: v1
kind: Service
metadata:
  name: backup-exporter-service
  namespace: zoptal-production
  labels:
    app: backup-exporter
spec:
  type: ClusterIP
  ports:
  - port: 9100
    targetPort: 9100
    name: metrics
  selector:
    app: backup-exporter