# Production Database Backup and Recovery Configuration
# Implements automated backups, monitoring, and recovery procedures

---
# PostgreSQL Backup Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-backup-config
  namespace: zoptal-production
  labels:
    app.kubernetes.io/name: postgres-backup
    app.kubernetes.io/component: config
data:
  backup.sh: |
    #!/bin/bash
    set -euo pipefail
    
    # Configuration
    TIMESTAMP=$(date +%Y%m%d_%H%M%S)
    BACKUP_DIR="/backups"
    RETENTION_DAYS=${BACKUP_RETENTION_DAYS:-7}
    S3_BUCKET=${S3_BACKUP_BUCKET:-"zoptal-db-backups"}
    
    # Create backup directory
    mkdir -p "$BACKUP_DIR"
    
    # Function to log messages
    log() {
        echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
    }
    
    # Function to perform database backup
    backup_database() {
        local db_name=$1
        local backup_file="$BACKUP_DIR/${db_name}_${TIMESTAMP}.sql"
        
        log "Starting backup of database: $db_name"
        
        # Create database dump
        pg_dump \
            --host="$POSTGRES_HOST" \
            --port="$POSTGRES_PORT" \
            --username="$POSTGRES_USER" \
            --dbname="$db_name" \
            --verbose \
            --clean \
            --if-exists \
            --create \
            --format=custom \
            --compress=9 \
            --file="$backup_file.dump"
        
        # Create SQL backup as well
        pg_dump \
            --host="$POSTGRES_HOST" \
            --port="$POSTGRES_PORT" \
            --username="$POSTGRES_USER" \
            --dbname="$db_name" \
            --verbose \
            --clean \
            --if-exists \
            --create \
            --file="$backup_file"
        
        # Compress SQL backup
        gzip "$backup_file"
        
        log "Backup completed: $backup_file.dump and $backup_file.gz"
        
        # Upload to S3 if configured
        if command -v aws &> /dev/null && [ -n "${AWS_ACCESS_KEY_ID:-}" ]; then
            log "Uploading backup to S3..."
            aws s3 cp "$backup_file.dump" "s3://$S3_BUCKET/postgres/$db_name/"
            aws s3 cp "$backup_file.gz" "s3://$S3_BUCKET/postgres/$db_name/"
            log "Backup uploaded to S3"
        fi
        
        return 0
    }
    
    # Function to clean old backups
    cleanup_old_backups() {
        log "Cleaning up backups older than $RETENTION_DAYS days"
        
        # Local cleanup
        find "$BACKUP_DIR" -name "*.sql.gz" -mtime +$RETENTION_DAYS -delete
        find "$BACKUP_DIR" -name "*.dump" -mtime +$RETENTION_DAYS -delete
        
        # S3 cleanup if configured
        if command -v aws &> /dev/null && [ -n "${AWS_ACCESS_KEY_ID:-}" ]; then
            aws s3api list-objects-v2 \
                --bucket "$S3_BUCKET" \
                --prefix "postgres/" \
                --query "Contents[?LastModified<='$(date -d "$RETENTION_DAYS days ago" --iso-8601)'].Key" \
                --output text | \
            xargs -r -I {} aws s3 rm "s3://$S3_BUCKET/{}"
        fi
        
        log "Cleanup completed"
    }
    
    # Function to verify backup integrity
    verify_backup() {
        local backup_file=$1
        
        log "Verifying backup integrity: $(basename $backup_file)"
        
        # Check if file exists and is not empty
        if [ ! -s "$backup_file" ]; then
            log "ERROR: Backup file is empty or doesn't exist"
            return 1
        fi
        
        # Verify dump file integrity
        if [[ "$backup_file" == *.dump ]]; then
            pg_restore --list "$backup_file" > /dev/null
        fi
        
        log "Backup verification successful"
        return 0
    }
    
    # Main backup process
    main() {
        log "Starting database backup process"
        
        # List of databases to backup
        DATABASES=${BACKUP_DATABASES:-"zoptal zoptal_auth zoptal_projects zoptal_billing"}
        
        for db in $DATABASES; do
            if backup_database "$db"; then
                verify_backup "$BACKUP_DIR/${db}_${TIMESTAMP}.dump"
            else
                log "ERROR: Failed to backup database $db"
                exit 1
            fi
        done
        
        # Cleanup old backups
        cleanup_old_backups
        
        # Send notification about backup status
        if [ -n "${WEBHOOK_URL:-}" ]; then
            curl -X POST "$WEBHOOK_URL" \
                -H "Content-Type: application/json" \
                -d "{\"text\":\"Database backup completed successfully at $(date)\"}" || true
        fi
        
        log "Database backup process completed successfully"
    }
    
    # Execute main function
    main "$@"

  restore.sh: |
    #!/bin/bash
    set -euo pipefail
    
    # Configuration
    BACKUP_DIR="/backups"
    S3_BUCKET=${S3_BACKUP_BUCKET:-"zoptal-db-backups"}
    
    # Function to log messages
    log() {
        echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
    }
    
    # Function to list available backups
    list_backups() {
        local db_name=${1:-""}
        
        log "Available backups:"
        
        if [ -n "$db_name" ]; then
            find "$BACKUP_DIR" -name "${db_name}_*.dump" -o -name "${db_name}_*.sql.gz" | sort -r
        else
            find "$BACKUP_DIR" -name "*.dump" -o -name "*.sql.gz" | sort -r
        fi
        
        # Also check S3 if configured
        if command -v aws &> /dev/null && [ -n "${AWS_ACCESS_KEY_ID:-}" ]; then
            log "S3 backups:"
            if [ -n "$db_name" ]; then
                aws s3 ls "s3://$S3_BUCKET/postgres/$db_name/" --recursive
            else
                aws s3 ls "s3://$S3_BUCKET/postgres/" --recursive
            fi
        fi
    }
    
    # Function to download backup from S3
    download_from_s3() {
        local s3_path=$1
        local local_path=$2
        
        log "Downloading backup from S3: $s3_path"
        aws s3 cp "$s3_path" "$local_path"
        log "Download completed: $local_path"
    }
    
    # Function to restore database
    restore_database() {
        local db_name=$1
        local backup_file=$2
        local target_db=${3:-$db_name}
        
        log "Starting restore of database: $db_name from $backup_file to $target_db"
        
        # Verify backup file exists
        if [ ! -f "$backup_file" ]; then
            log "ERROR: Backup file not found: $backup_file"
            return 1
        fi
        
        # Create target database if it doesn't exist
        log "Creating target database: $target_db"
        createdb \
            --host="$POSTGRES_HOST" \
            --port="$POSTGRES_PORT" \
            --username="$POSTGRES_USER" \
            "$target_db" || log "Database $target_db already exists"
        
        # Restore database
        if [[ "$backup_file" == *.dump ]]; then
            # Custom format restore
            pg_restore \
                --host="$POSTGRES_HOST" \
                --port="$POSTGRES_PORT" \
                --username="$POSTGRES_USER" \
                --dbname="$target_db" \
                --verbose \
                --clean \
                --if-exists \
                --create \
                --exit-on-error \
                "$backup_file"
        elif [[ "$backup_file" == *.sql.gz ]]; then
            # SQL format restore
            gunzip -c "$backup_file" | psql \
                --host="$POSTGRES_HOST" \
                --port="$POSTGRES_PORT" \
                --username="$POSTGRES_USER" \
                --dbname="$target_db"
        else
            log "ERROR: Unsupported backup format: $backup_file"
            return 1
        fi
        
        log "Database restore completed successfully"
    }
    
    # Function to perform point-in-time recovery
    point_in_time_recovery() {
        local target_time=$1
        local target_db=$2
        
        log "Starting point-in-time recovery to: $target_time"
        
        # This would require WAL archiving to be enabled
        # Implementation depends on your WAL archive setup
        log "Point-in-time recovery requires WAL archiving configuration"
        log "Please refer to PostgreSQL documentation for PITR setup"
    }
    
    # Main restore function
    main() {
        local action=${1:-"list"}
        
        case "$action" in
            "list")
                list_backups "${2:-}"
                ;;
            "restore")
                local db_name=${2:-}
                local backup_file=${3:-}
                local target_db=${4:-$db_name}
                
                if [ -z "$db_name" ] || [ -z "$backup_file" ]; then
                    log "Usage: $0 restore <db_name> <backup_file> [target_db]"
                    exit 1
                fi
                
                restore_database "$db_name" "$backup_file" "$target_db"
                ;;
            "download")
                local s3_path=${2:-}
                local local_path=${3:-}
                
                if [ -z "$s3_path" ] || [ -z "$local_path" ]; then
                    log "Usage: $0 download <s3_path> <local_path>"
                    exit 1
                fi
                
                download_from_s3 "$s3_path" "$local_path"
                ;;
            "pitr")
                local target_time=${2:-}
                local target_db=${3:-}
                
                if [ -z "$target_time" ] || [ -z "$target_db" ]; then
                    log "Usage: $0 pitr <target_time> <target_db>"
                    exit 1
                fi
                
                point_in_time_recovery "$target_time" "$target_db"
                ;;
            *)
                log "Usage: $0 [list|restore|download|pitr] [options...]"
                log "  list [db_name]                    - List available backups"
                log "  restore <db> <backup> [target]    - Restore database from backup"
                log "  download <s3_path> <local_path>   - Download backup from S3"
                log "  pitr <time> <target_db>           - Point-in-time recovery"
                exit 1
                ;;
        esac
    }
    
    # Execute main function
    main "$@"

---
# Backup CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
  namespace: zoptal-production
  labels:
    app.kubernetes.io/name: postgres-backup
    app.kubernetes.io/component: backup-job
spec:
  # Run daily at 2 AM UTC
  schedule: "0 2 * * *"
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 3
      activeDeadlineSeconds: 3600 # 1 hour timeout
      template:
        metadata:
          labels:
            app.kubernetes.io/name: postgres-backup
            app.kubernetes.io/component: backup-job
        spec:
          restartPolicy: OnFailure
          serviceAccountName: postgres-backup
          securityContext:
            runAsNonRoot: true
            runAsUser: 999
            fsGroup: 999
          containers:
          - name: postgres-backup
            image: postgres:15-alpine
            command: ["/bin/bash", "/scripts/backup.sh"]
            env:
            - name: POSTGRES_HOST
              value: "postgres.zoptal-production.svc.cluster.local"
            - name: POSTGRES_PORT
              value: "5432"
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: postgres-credentials
                  key: username
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-credentials
                  key: password
            - name: BACKUP_DATABASES
              value: "zoptal zoptal_auth zoptal_projects zoptal_billing"
            - name: BACKUP_RETENTION_DAYS
              value: "30"
            - name: S3_BACKUP_BUCKET
              value: "zoptal-db-backups-prod"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-backup-credentials
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-backup-credentials
                  key: secret-access-key
            - name: AWS_DEFAULT_REGION
              value: "us-east-1"
            - name: WEBHOOK_URL
              valueFrom:
                secretKeyRef:
                  name: backup-notifications
                  key: webhook-url
                  optional: true
            volumeMounts:
            - name: backup-scripts
              mountPath: /scripts
              readOnly: true
            - name: backup-storage
              mountPath: /backups
            resources:
              requests:
                memory: "512Mi"
                cpu: "250m"
              limits:
                memory: "1Gi"
                cpu: "500m"
          volumes:
          - name: backup-scripts
            configMap:
              name: postgres-backup-config
              defaultMode: 0755
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-storage-pvc

---
# Backup Storage PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-storage-pvc
  namespace: zoptal-production
  labels:
    app.kubernetes.io/name: postgres-backup
    app.kubernetes.io/component: storage
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 100Gi

---
# Service Account for Backup Job
apiVersion: v1
kind: ServiceAccount
metadata:
  name: postgres-backup
  namespace: zoptal-production
  labels:
    app.kubernetes.io/name: postgres-backup
    app.kubernetes.io/component: service-account

---
# Role for Backup Job
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: postgres-backup-role
  namespace: zoptal-production
  labels:
    app.kubernetes.io/name: postgres-backup
    app.kubernetes.io/component: rbac
rules:
- apiGroups: [""]
  resources: ["secrets", "configmaps"]
  verbs: ["get", "list"]
  resourceNames: ["postgres-credentials", "s3-backup-credentials", "backup-notifications"]

---
# RoleBinding for Backup Job
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: postgres-backup-binding
  namespace: zoptal-production
  labels:
    app.kubernetes.io/name: postgres-backup
    app.kubernetes.io/component: rbac
subjects:
- kind: ServiceAccount
  name: postgres-backup
  namespace: zoptal-production
roleRef:
  kind: Role
  name: postgres-backup-role
  apiGroup: rbac.authorization.k8s.io

---
# Redis Backup CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: redis-backup
  namespace: zoptal-production
  labels:
    app.kubernetes.io/name: redis-backup
    app.kubernetes.io/component: backup-job
spec:
  # Run every 6 hours
  schedule: "0 */6 * * *"
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 3
      activeDeadlineSeconds: 1800 # 30 minutes timeout
      template:
        metadata:
          labels:
            app.kubernetes.io/name: redis-backup
            app.kubernetes.io/component: backup-job
        spec:
          restartPolicy: OnFailure
          serviceAccountName: redis-backup
          securityContext:
            runAsNonRoot: true
            runAsUser: 999
            fsGroup: 999
          containers:
          - name: redis-backup
            image: redis:7-alpine
            command:
            - /bin/sh
            - -c
            - |
              set -e
              TIMESTAMP=$(date +%Y%m%d_%H%M%S)
              BACKUP_FILE="/backups/redis_${TIMESTAMP}.rdb"
              
              echo "Starting Redis backup at $(date)"
              
              # Create Redis dump
              redis-cli -h redis.zoptal-production.svc.cluster.local -p 6379 --rdb "$BACKUP_FILE"
              
              # Compress backup
              gzip "$BACKUP_FILE"
              
              # Upload to S3 if configured
              if [ -n "${AWS_ACCESS_KEY_ID:-}" ]; then
                aws s3 cp "${BACKUP_FILE}.gz" "s3://${S3_BACKUP_BUCKET}/redis/"
                echo "Backup uploaded to S3"
              fi
              
              # Cleanup old backups (keep last 7 days)
              find /backups -name "redis_*.rdb.gz" -mtime +7 -delete
              
              echo "Redis backup completed successfully"
            env:
            - name: S3_BACKUP_BUCKET
              value: "zoptal-db-backups-prod"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-backup-credentials
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-backup-credentials
                  key: secret-access-key
            - name: AWS_DEFAULT_REGION
              value: "us-east-1"
            volumeMounts:
            - name: backup-storage
              mountPath: /backups
            resources:
              requests:
                memory: "256Mi"
                cpu: "125m"
              limits:
                memory: "512Mi"
                cpu: "250m"
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-storage-pvc

---
# Service Account for Redis Backup
apiVersion: v1
kind: ServiceAccount
metadata:
  name: redis-backup
  namespace: zoptal-production
  labels:
    app.kubernetes.io/name: redis-backup
    app.kubernetes.io/component: service-account

---
# Backup Monitoring ServiceMonitor
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: backup-monitoring
  namespace: zoptal-production
  labels:
    app.kubernetes.io/name: backup-monitoring
    app.kubernetes.io/component: monitoring
    release: prometheus
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: backup-exporter
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

---
# Backup Metrics Exporter
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backup-exporter
  namespace: zoptal-production
  labels:
    app.kubernetes.io/name: backup-exporter
    app.kubernetes.io/component: metrics
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: backup-exporter
  template:
    metadata:
      labels:
        app.kubernetes.io/name: backup-exporter
        app.kubernetes.io/component: metrics
    spec:
      serviceAccountName: backup-exporter
      containers:
      - name: backup-exporter
        image: prom/node-exporter:latest
        ports:
        - containerPort: 9100
          name: metrics
        volumeMounts:
        - name: backup-storage
          mountPath: /backups
          readOnly: true
        command:
        - /bin/sh
        - -c
        - |
          # Custom backup metrics exporter
          cat > /tmp/backup_metrics.prom << 'EOF'
          # HELP backup_last_success_timestamp_seconds Last successful backup timestamp
          # TYPE backup_last_success_timestamp_seconds gauge
          backup_last_success_timestamp_seconds{service="postgres"} $(stat -c %Y /backups/postgres_*.dump 2>/dev/null | tail -1 || echo 0)
          backup_last_success_timestamp_seconds{service="redis"} $(stat -c %Y /backups/redis_*.rdb.gz 2>/dev/null | tail -1 || echo 0)
          
          # HELP backup_file_size_bytes Size of the latest backup file
          # TYPE backup_file_size_bytes gauge
          backup_file_size_bytes{service="postgres"} $(stat -c %s /backups/postgres_*.dump 2>/dev/null | tail -1 || echo 0)
          backup_file_size_bytes{service="redis"} $(stat -c %s /backups/redis_*.rdb.gz 2>/dev/null | tail -1 || echo 0)
          
          # HELP backup_files_total Total number of backup files
          # TYPE backup_files_total gauge
          backup_files_total{service="postgres"} $(ls /backups/postgres_*.dump 2>/dev/null | wc -l)
          backup_files_total{service="redis"} $(ls /backups/redis_*.rdb.gz 2>/dev/null | wc -l)
          EOF
          
          # Start HTTP server to serve metrics
          while true; do
            # Update metrics
            cat > /tmp/backup_metrics.prom << 'EOF'
          backup_last_success_timestamp_seconds{service="postgres"} $(stat -c %Y /backups/postgres_*.dump 2>/dev/null | tail -1 || echo 0)
          backup_last_success_timestamp_seconds{service="redis"} $(stat -c %Y /backups/redis_*.rdb.gz 2>/dev/null | tail -1 || echo 0)
          backup_file_size_bytes{service="postgres"} $(stat -c %s /backups/postgres_*.dump 2>/dev/null | tail -1 || echo 0)
          backup_file_size_bytes{service="redis"} $(stat -c %s /backups/redis_*.rdb.gz 2>/dev/null | tail -1 || echo 0)
          backup_files_total{service="postgres"} $(ls /backups/postgres_*.dump 2>/dev/null | wc -l)
          backup_files_total{service="redis"} $(ls /backups/redis_*.rdb.gz 2>/dev/null | wc -l)
          EOF
            
            # Serve metrics via netcat (simple HTTP server)
            echo -e "HTTP/1.1 200 OK\r\nContent-Type: text/plain\r\n\r\n$(cat /tmp/backup_metrics.prom)" | nc -l -p 9100 -q 1
            sleep 10
          done
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
      volumes:
      - name: backup-storage
        persistentVolumeClaim:
          claimName: backup-storage-pvc

---
# Service Account for Backup Exporter
apiVersion: v1
kind: ServiceAccount
metadata:
  name: backup-exporter
  namespace: zoptal-production
  labels:
    app.kubernetes.io/name: backup-exporter
    app.kubernetes.io/component: service-account

---
# Service for Backup Exporter
apiVersion: v1
kind: Service
metadata:
  name: backup-exporter
  namespace: zoptal-production
  labels:
    app.kubernetes.io/name: backup-exporter
    app.kubernetes.io/component: metrics
spec:
  ports:
  - port: 9100
    name: metrics
  selector:
    app.kubernetes.io/name: backup-exporter